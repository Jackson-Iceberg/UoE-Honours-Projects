{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5d67fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "from patchify import patchify\n",
    "import PIL\n",
    "from PIL import Image\n",
    "PIL.Image.MAX_IMAGE_PIXELS = 933120000\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "%matplotlib inline\n",
    "import torchvision\n",
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1698e87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchDataset(Dataset):\n",
    "    def __init__(self,root,target, train=True, transforms=None):\n",
    "        super(PatchDataset, self).__init__()\n",
    "        self.image_path = [os.path.join(root, x) for x in os.listdir(root)]      \n",
    "        self.ref_path = [os.path.join(target,x) for x in os.listdir(target)]\n",
    "        \n",
    "        if transform is not None:\n",
    "            self.transform = transform\n",
    "\n",
    "        if train:\n",
    "            self.images = self.image_path[: int(.8 * len(self.image_path))]\n",
    "            self.ref = self.ref_path[: int(.8 * len(self.image_path))]\n",
    "        else:\n",
    "            self.images = self.image_path[int(.8 * len(self.image_path)):]\n",
    "            self.ref = self.ref_path[int(.8 * len(self.image_path)):]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.transform(self.images[item]),self.transform(self.ref[item])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afd32a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    lambda x: Image.open(x).convert('RGB'),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b0ae269",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.encoder_ = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder_(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3c73b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.decoder_ = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2, padding=0),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2, padding=0),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.ConvTranspose2d(32, 3, kernel_size=2, stride=2, padding=0),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder_(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69e0540b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CAE, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9012cfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedContrastiveLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModifiedContrastiveLoss, self).__init__()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "    def forward(self, encoded_x1, encoded_x2):\n",
    "        # Calculate the modified contrastive loss using MSE loss\n",
    "        loss = self.mse_loss(encoded_x1, encoded_x2)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5854a55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseCAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseCAE, self).__init__()\n",
    "        self.cae = CAE()\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # Pass the input images through the shared-weight CAE\n",
    "        encoded_x1 = self.cae.encoder(x1)\n",
    "        encoded_x2 = self.cae.encoder(x2)\n",
    "\n",
    "        # Decode the encoded representations\n",
    "        decoded_x1 = self.cae.decoder(encoded_x1)\n",
    "        decoded_x2 = self.cae.decoder(encoded_x2)\n",
    "\n",
    "        return encoded_x1, encoded_x2, decoded_x1, decoded_x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9ae0108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "SiameseCAE(\n",
      "  (cae): CAE(\n",
      "    (encoder): Encoder(\n",
      "      (encoder_): Sequential(\n",
      "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (6): ReLU()\n",
      "        (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (10): ReLU()\n",
      "        (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "    )\n",
      "    (decoder): Decoder(\n",
      "      (decoder_): Sequential(\n",
      "        (0): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
      "        (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU()\n",
      "        (6): ConvTranspose2d(32, 3, kernel_size=(2, 2), stride=(2, 2))\n",
      "        (7): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = CAE().to(device)\n",
    "# Set loss function and optimizer\n",
    "criterion = nn.MSELoss().to(device)\n",
    "l1_loss = nn.L1Loss().to(device)\n",
    "Hube = torch.nn.HuberLoss().to(device)\n",
    "CE = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "model = SiameseCAE().to(device)\n",
    "contrastive_loss = ModifiedContrastiveLoss()\n",
    "reconstruction_loss = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "unloader = transforms.ToPILImage()\n",
    "\n",
    "train_dataset = PatchDataset(\"OS_412(64)_SSIM_BlankRemove\",\"OS_415(64)_SSIM_BlankRemove\",train=True, transforms=transform)\n",
    "test_dataset = PatchDataset(\"OS_412(64)_SSIM_BlankRemove\",\"OS_415(64)_SSIM_BlankRemove\",train=False, transforms=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(device)\n",
    "print(model.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1c689d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Train Loss: 0.0456\n",
      "Epoch [1/500], Test Loss: 0.0470\n",
      "Epoch [2/500], Train Loss: 0.0360\n",
      "Epoch [2/500], Test Loss: 0.0366\n",
      "Epoch [3/500], Train Loss: 0.0307\n",
      "Epoch [3/500], Test Loss: 0.0306\n",
      "Epoch [4/500], Train Loss: 0.0283\n",
      "Epoch [4/500], Test Loss: 0.0281\n",
      "Epoch [5/500], Train Loss: 0.0264\n",
      "Epoch [5/500], Test Loss: 0.0268\n",
      "Epoch [6/500], Train Loss: 0.0255\n",
      "Epoch [6/500], Test Loss: 0.0259\n",
      "Epoch [7/500], Train Loss: 0.0248\n",
      "Epoch [7/500], Test Loss: 0.0254\n",
      "Epoch [8/500], Train Loss: 0.0246\n",
      "Epoch [8/500], Test Loss: 0.0250\n",
      "Epoch [9/500], Train Loss: 0.0239\n",
      "Epoch [9/500], Test Loss: 0.0245\n",
      "Epoch [10/500], Train Loss: 0.0236\n",
      "Epoch [10/500], Test Loss: 0.0242\n",
      "Epoch [11/500], Train Loss: 0.0232\n",
      "Epoch [11/500], Test Loss: 0.0239\n",
      "Epoch [12/500], Train Loss: 0.0230\n",
      "Epoch [12/500], Test Loss: 0.0236\n",
      "Epoch [13/500], Train Loss: 0.0227\n",
      "Epoch [13/500], Test Loss: 0.0234\n",
      "Epoch [14/500], Train Loss: 0.0226\n",
      "Epoch [14/500], Test Loss: 0.0232\n",
      "Epoch [15/500], Train Loss: 0.0223\n",
      "Epoch [15/500], Test Loss: 0.0229\n",
      "Epoch [16/500], Train Loss: 0.0220\n",
      "Epoch [16/500], Test Loss: 0.0228\n",
      "Epoch [17/500], Train Loss: 0.0218\n",
      "Epoch [17/500], Test Loss: 0.0226\n",
      "Epoch [18/500], Train Loss: 0.0218\n",
      "Epoch [18/500], Test Loss: 0.0225\n",
      "Epoch [19/500], Train Loss: 0.0214\n",
      "Epoch [19/500], Test Loss: 0.0223\n",
      "Epoch [20/500], Train Loss: 0.0210\n",
      "Epoch [20/500], Test Loss: 0.0222\n",
      "Epoch [21/500], Train Loss: 0.0207\n",
      "Epoch [21/500], Test Loss: 0.0220\n",
      "Epoch [22/500], Train Loss: 0.0205\n",
      "Epoch [22/500], Test Loss: 0.0219\n",
      "Epoch [23/500], Train Loss: 0.0203\n",
      "Epoch [23/500], Test Loss: 0.0218\n",
      "Epoch [24/500], Train Loss: 0.0203\n",
      "Epoch [24/500], Test Loss: 0.0217\n",
      "Epoch [25/500], Train Loss: 0.0200\n",
      "Epoch [25/500], Test Loss: 0.0216\n",
      "Epoch [26/500], Train Loss: 0.0200\n",
      "Epoch [26/500], Test Loss: 0.0215\n",
      "Epoch [27/500], Train Loss: 0.0198\n",
      "Epoch [27/500], Test Loss: 0.0213\n",
      "Epoch [28/500], Train Loss: 0.0197\n",
      "Epoch [28/500], Test Loss: 0.0213\n",
      "Epoch [29/500], Train Loss: 0.0202\n",
      "Epoch [29/500], Test Loss: 0.0213\n",
      "Epoch [30/500], Train Loss: 0.0196\n",
      "Epoch [30/500], Test Loss: 0.0211\n",
      "Epoch [31/500], Train Loss: 0.0195\n",
      "Epoch [31/500], Test Loss: 0.0211\n",
      "Epoch [32/500], Train Loss: 0.0196\n",
      "Epoch [32/500], Test Loss: 0.0211\n",
      "Epoch [33/500], Train Loss: 0.0194\n",
      "Epoch [33/500], Test Loss: 0.0211\n",
      "Epoch [34/500], Train Loss: 0.0193\n",
      "Epoch [34/500], Test Loss: 0.0209\n",
      "Epoch [35/500], Train Loss: 0.0192\n",
      "Epoch [35/500], Test Loss: 0.0209\n",
      "Epoch [36/500], Train Loss: 0.0193\n",
      "Epoch [36/500], Test Loss: 0.0208\n",
      "Epoch [37/500], Train Loss: 0.0193\n",
      "Epoch [37/500], Test Loss: 0.0208\n",
      "Epoch [38/500], Train Loss: 0.0191\n",
      "Epoch [38/500], Test Loss: 0.0207\n",
      "Epoch [39/500], Train Loss: 0.0191\n",
      "Epoch [39/500], Test Loss: 0.0206\n",
      "Epoch [40/500], Train Loss: 0.0189\n",
      "Epoch [40/500], Test Loss: 0.0206\n",
      "Epoch [41/500], Train Loss: 0.0189\n",
      "Epoch [41/500], Test Loss: 0.0205\n",
      "Epoch [42/500], Train Loss: 0.0189\n",
      "Epoch [42/500], Test Loss: 0.0205\n",
      "Epoch [43/500], Train Loss: 0.0188\n",
      "Epoch [43/500], Test Loss: 0.0204\n",
      "Epoch [44/500], Train Loss: 0.0188\n",
      "Epoch [44/500], Test Loss: 0.0204\n",
      "Epoch [45/500], Train Loss: 0.0187\n",
      "Epoch [45/500], Test Loss: 0.0204\n",
      "Epoch [46/500], Train Loss: 0.0187\n",
      "Epoch [46/500], Test Loss: 0.0203\n",
      "Epoch [47/500], Train Loss: 0.0187\n",
      "Epoch [47/500], Test Loss: 0.0203\n",
      "Epoch [48/500], Train Loss: 0.0186\n",
      "Epoch [48/500], Test Loss: 0.0202\n",
      "Epoch [49/500], Train Loss: 0.0186\n",
      "Epoch [49/500], Test Loss: 0.0202\n",
      "Epoch [50/500], Train Loss: 0.0185\n",
      "Epoch [50/500], Test Loss: 0.0202\n",
      "Epoch [51/500], Train Loss: 0.0185\n",
      "Epoch [51/500], Test Loss: 0.0201\n",
      "Epoch [52/500], Train Loss: 0.0185\n",
      "Epoch [52/500], Test Loss: 0.0201\n",
      "Epoch [53/500], Train Loss: 0.0183\n",
      "Epoch [53/500], Test Loss: 0.0201\n",
      "Epoch [54/500], Train Loss: 0.0183\n",
      "Epoch [54/500], Test Loss: 0.0200\n",
      "Epoch [55/500], Train Loss: 0.0183\n",
      "Epoch [55/500], Test Loss: 0.0200\n",
      "Epoch [56/500], Train Loss: 0.0183\n",
      "Epoch [56/500], Test Loss: 0.0200\n",
      "Epoch [57/500], Train Loss: 0.0182\n",
      "Epoch [57/500], Test Loss: 0.0199\n",
      "Epoch [58/500], Train Loss: 0.0186\n",
      "Epoch [58/500], Test Loss: 0.0199\n",
      "Epoch [59/500], Train Loss: 0.0182\n",
      "Epoch [59/500], Test Loss: 0.0199\n",
      "Epoch [60/500], Train Loss: 0.0181\n",
      "Epoch [60/500], Test Loss: 0.0198\n",
      "Epoch [61/500], Train Loss: 0.0181\n",
      "Epoch [61/500], Test Loss: 0.0198\n",
      "Epoch [62/500], Train Loss: 0.0181\n",
      "Epoch [62/500], Test Loss: 0.0198\n",
      "Epoch [63/500], Train Loss: 0.0181\n",
      "Epoch [63/500], Test Loss: 0.0198\n",
      "Epoch [64/500], Train Loss: 0.0180\n",
      "Epoch [64/500], Test Loss: 0.0196\n",
      "Epoch [65/500], Train Loss: 0.0180\n",
      "Epoch [65/500], Test Loss: 0.0196\n",
      "Epoch [66/500], Train Loss: 0.0180\n",
      "Epoch [66/500], Test Loss: 0.0196\n",
      "Epoch [67/500], Train Loss: 0.0180\n",
      "Epoch [67/500], Test Loss: 0.0196\n",
      "Epoch [68/500], Train Loss: 0.0180\n",
      "Epoch [68/500], Test Loss: 0.0196\n",
      "Epoch [69/500], Train Loss: 0.0180\n",
      "Epoch [69/500], Test Loss: 0.0195\n",
      "Epoch [70/500], Train Loss: 0.0179\n",
      "Epoch [70/500], Test Loss: 0.0195\n",
      "Epoch [71/500], Train Loss: 0.0179\n",
      "Epoch [71/500], Test Loss: 0.0195\n",
      "Epoch [72/500], Train Loss: 0.0180\n",
      "Epoch [72/500], Test Loss: 0.0194\n",
      "Epoch [73/500], Train Loss: 0.0178\n",
      "Epoch [73/500], Test Loss: 0.0194\n",
      "Epoch [74/500], Train Loss: 0.0178\n",
      "Epoch [74/500], Test Loss: 0.0193\n",
      "Epoch [75/500], Train Loss: 0.0178\n",
      "Epoch [75/500], Test Loss: 0.0193\n",
      "Epoch [76/500], Train Loss: 0.0178\n",
      "Epoch [76/500], Test Loss: 0.0193\n",
      "Epoch [77/500], Train Loss: 0.0178\n",
      "Epoch [77/500], Test Loss: 0.0193\n",
      "Epoch [78/500], Train Loss: 0.0177\n",
      "Epoch [78/500], Test Loss: 0.0192\n",
      "Epoch [79/500], Train Loss: 0.0177\n",
      "Epoch [79/500], Test Loss: 0.0192\n",
      "Epoch [80/500], Train Loss: 0.0177\n",
      "Epoch [80/500], Test Loss: 0.0192\n",
      "Epoch [81/500], Train Loss: 0.0176\n",
      "Epoch [81/500], Test Loss: 0.0191\n",
      "Epoch [82/500], Train Loss: 0.0176\n",
      "Epoch [82/500], Test Loss: 0.0191\n",
      "Epoch [83/500], Train Loss: 0.0177\n",
      "Epoch [83/500], Test Loss: 0.0191\n",
      "Epoch [84/500], Train Loss: 0.0177\n",
      "Epoch [84/500], Test Loss: 0.0191\n",
      "Epoch [85/500], Train Loss: 0.0176\n",
      "Epoch [85/500], Test Loss: 0.0190\n",
      "Epoch [86/500], Train Loss: 0.0177\n",
      "Epoch [86/500], Test Loss: 0.0191\n",
      "Epoch [87/500], Train Loss: 0.0177\n",
      "Epoch [87/500], Test Loss: 0.0190\n",
      "Epoch [88/500], Train Loss: 0.0176\n",
      "Epoch [88/500], Test Loss: 0.0190\n",
      "Epoch [89/500], Train Loss: 0.0176\n",
      "Epoch [89/500], Test Loss: 0.0190\n",
      "Epoch [90/500], Train Loss: 0.0175\n",
      "Epoch [90/500], Test Loss: 0.0190\n",
      "Epoch [91/500], Train Loss: 0.0175\n",
      "Epoch [91/500], Test Loss: 0.0189\n",
      "Epoch [92/500], Train Loss: 0.0175\n",
      "Epoch [92/500], Test Loss: 0.0189\n",
      "Epoch [93/500], Train Loss: 0.0175\n",
      "Epoch [93/500], Test Loss: 0.0189\n",
      "Epoch [94/500], Train Loss: 0.0175\n",
      "Epoch [94/500], Test Loss: 0.0189\n",
      "Epoch [95/500], Train Loss: 0.0175\n",
      "Epoch [95/500], Test Loss: 0.0189\n",
      "Epoch [96/500], Train Loss: 0.0174\n",
      "Epoch [96/500], Test Loss: 0.0189\n",
      "Epoch [97/500], Train Loss: 0.0175\n",
      "Epoch [97/500], Test Loss: 0.0189\n",
      "Epoch [98/500], Train Loss: 0.0175\n",
      "Epoch [98/500], Test Loss: 0.0188\n",
      "Epoch [99/500], Train Loss: 0.0175\n",
      "Epoch [99/500], Test Loss: 0.0189\n",
      "Epoch [100/500], Train Loss: 0.0174\n",
      "Epoch [100/500], Test Loss: 0.0188\n",
      "Epoch [101/500], Train Loss: 0.0174\n",
      "Epoch [101/500], Test Loss: 0.0188\n",
      "Epoch [102/500], Train Loss: 0.0174\n",
      "Epoch [102/500], Test Loss: 0.0188\n",
      "Epoch [103/500], Train Loss: 0.0174\n",
      "Epoch [103/500], Test Loss: 0.0188\n",
      "Epoch [104/500], Train Loss: 0.0174\n",
      "Epoch [104/500], Test Loss: 0.0188\n",
      "Epoch [105/500], Train Loss: 0.0174\n",
      "Epoch [105/500], Test Loss: 0.0188\n",
      "Epoch [106/500], Train Loss: 0.0174\n",
      "Epoch [106/500], Test Loss: 0.0188\n",
      "Epoch [107/500], Train Loss: 0.0174\n",
      "Epoch [107/500], Test Loss: 0.0188\n",
      "Epoch [108/500], Train Loss: 0.0173\n",
      "Epoch [108/500], Test Loss: 0.0187\n",
      "Epoch [109/500], Train Loss: 0.0174\n",
      "Epoch [109/500], Test Loss: 0.0187\n",
      "Epoch [110/500], Train Loss: 0.0174\n",
      "Epoch [110/500], Test Loss: 0.0187\n",
      "Epoch [111/500], Train Loss: 0.0173\n",
      "Epoch [111/500], Test Loss: 0.0187\n",
      "Epoch [112/500], Train Loss: 0.0173\n",
      "Epoch [112/500], Test Loss: 0.0187\n",
      "Epoch [113/500], Train Loss: 0.0173\n",
      "Epoch [113/500], Test Loss: 0.0187\n",
      "Epoch [114/500], Train Loss: 0.0173\n",
      "Epoch [114/500], Test Loss: 0.0187\n",
      "Epoch [115/500], Train Loss: 0.0173\n",
      "Epoch [115/500], Test Loss: 0.0187\n",
      "Epoch [116/500], Train Loss: 0.0173\n",
      "Epoch [116/500], Test Loss: 0.0186\n",
      "Epoch [117/500], Train Loss: 0.0173\n",
      "Epoch [117/500], Test Loss: 0.0187\n",
      "Epoch [118/500], Train Loss: 0.0173\n",
      "Epoch [118/500], Test Loss: 0.0187\n",
      "Epoch [119/500], Train Loss: 0.0173\n",
      "Epoch [119/500], Test Loss: 0.0187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [120/500], Train Loss: 0.0173\n",
      "Epoch [120/500], Test Loss: 0.0186\n",
      "Epoch [121/500], Train Loss: 0.0172\n",
      "Epoch [121/500], Test Loss: 0.0186\n",
      "Epoch [122/500], Train Loss: 0.0172\n",
      "Epoch [122/500], Test Loss: 0.0186\n",
      "Epoch [123/500], Train Loss: 0.0172\n",
      "Epoch [123/500], Test Loss: 0.0186\n",
      "Epoch [124/500], Train Loss: 0.0172\n",
      "Epoch [124/500], Test Loss: 0.0186\n",
      "Epoch [125/500], Train Loss: 0.0173\n",
      "Epoch [125/500], Test Loss: 0.0186\n",
      "Epoch [126/500], Train Loss: 0.0172\n",
      "Epoch [126/500], Test Loss: 0.0186\n",
      "Epoch [127/500], Train Loss: 0.0172\n",
      "Epoch [127/500], Test Loss: 0.0185\n",
      "Epoch [128/500], Train Loss: 0.0172\n",
      "Epoch [128/500], Test Loss: 0.0185\n",
      "Epoch [129/500], Train Loss: 0.0172\n",
      "Epoch [129/500], Test Loss: 0.0185\n",
      "Epoch [130/500], Train Loss: 0.0173\n",
      "Epoch [130/500], Test Loss: 0.0186\n",
      "Epoch [131/500], Train Loss: 0.0172\n",
      "Epoch [131/500], Test Loss: 0.0185\n",
      "Epoch [132/500], Train Loss: 0.0172\n",
      "Epoch [132/500], Test Loss: 0.0185\n",
      "Epoch [133/500], Train Loss: 0.0172\n",
      "Epoch [133/500], Test Loss: 0.0185\n",
      "Epoch [134/500], Train Loss: 0.0172\n",
      "Epoch [134/500], Test Loss: 0.0185\n",
      "Epoch [135/500], Train Loss: 0.0172\n",
      "Epoch [135/500], Test Loss: 0.0185\n",
      "Epoch [136/500], Train Loss: 0.0171\n",
      "Epoch [136/500], Test Loss: 0.0185\n",
      "Epoch [137/500], Train Loss: 0.0171\n",
      "Epoch [137/500], Test Loss: 0.0185\n",
      "Epoch [138/500], Train Loss: 0.0172\n",
      "Epoch [138/500], Test Loss: 0.0185\n",
      "Epoch [139/500], Train Loss: 0.0171\n",
      "Epoch [139/500], Test Loss: 0.0185\n",
      "Epoch [140/500], Train Loss: 0.0171\n",
      "Epoch [140/500], Test Loss: 0.0184\n",
      "Epoch [141/500], Train Loss: 0.0171\n",
      "Epoch [141/500], Test Loss: 0.0185\n",
      "Epoch [142/500], Train Loss: 0.0171\n",
      "Epoch [142/500], Test Loss: 0.0185\n",
      "Epoch [143/500], Train Loss: 0.0171\n",
      "Epoch [143/500], Test Loss: 0.0185\n",
      "Epoch [144/500], Train Loss: 0.0171\n",
      "Epoch [144/500], Test Loss: 0.0184\n",
      "Epoch [145/500], Train Loss: 0.0171\n",
      "Epoch [145/500], Test Loss: 0.0184\n",
      "Epoch [146/500], Train Loss: 0.0171\n",
      "Epoch [146/500], Test Loss: 0.0184\n",
      "Epoch [147/500], Train Loss: 0.0171\n",
      "Epoch [147/500], Test Loss: 0.0184\n",
      "Epoch [148/500], Train Loss: 0.0171\n",
      "Epoch [148/500], Test Loss: 0.0184\n",
      "Epoch [149/500], Train Loss: 0.0171\n",
      "Epoch [149/500], Test Loss: 0.0184\n",
      "Epoch [150/500], Train Loss: 0.0171\n",
      "Epoch [150/500], Test Loss: 0.0184\n",
      "Epoch [151/500], Train Loss: 0.0171\n",
      "Epoch [151/500], Test Loss: 0.0184\n",
      "Epoch [152/500], Train Loss: 0.0171\n",
      "Epoch [152/500], Test Loss: 0.0184\n",
      "Epoch [153/500], Train Loss: 0.0170\n",
      "Epoch [153/500], Test Loss: 0.0184\n",
      "Epoch [154/500], Train Loss: 0.0171\n",
      "Epoch [154/500], Test Loss: 0.0184\n",
      "Epoch [155/500], Train Loss: 0.0171\n",
      "Epoch [155/500], Test Loss: 0.0184\n",
      "Epoch [156/500], Train Loss: 0.0171\n",
      "Epoch [156/500], Test Loss: 0.0184\n",
      "Epoch [157/500], Train Loss: 0.0170\n",
      "Epoch [157/500], Test Loss: 0.0184\n",
      "Epoch [158/500], Train Loss: 0.0170\n",
      "Epoch [158/500], Test Loss: 0.0184\n",
      "Epoch [159/500], Train Loss: 0.0170\n",
      "Epoch [159/500], Test Loss: 0.0184\n",
      "Epoch [160/500], Train Loss: 0.0170\n",
      "Epoch [160/500], Test Loss: 0.0184\n",
      "Epoch [161/500], Train Loss: 0.0170\n",
      "Epoch [161/500], Test Loss: 0.0183\n",
      "Epoch [162/500], Train Loss: 0.0170\n",
      "Epoch [162/500], Test Loss: 0.0184\n",
      "Epoch [163/500], Train Loss: 0.0170\n",
      "Epoch [163/500], Test Loss: 0.0183\n",
      "Epoch [164/500], Train Loss: 0.0170\n",
      "Epoch [164/500], Test Loss: 0.0184\n",
      "Epoch [165/500], Train Loss: 0.0170\n",
      "Epoch [165/500], Test Loss: 0.0183\n",
      "Epoch [166/500], Train Loss: 0.0170\n",
      "Epoch [166/500], Test Loss: 0.0183\n",
      "Epoch [167/500], Train Loss: 0.0170\n",
      "Epoch [167/500], Test Loss: 0.0183\n",
      "Epoch [168/500], Train Loss: 0.0170\n",
      "Epoch [168/500], Test Loss: 0.0183\n",
      "Epoch [169/500], Train Loss: 0.0170\n",
      "Epoch [169/500], Test Loss: 0.0183\n",
      "Epoch [170/500], Train Loss: 0.0170\n",
      "Epoch [170/500], Test Loss: 0.0183\n",
      "Epoch [171/500], Train Loss: 0.0170\n",
      "Epoch [171/500], Test Loss: 0.0183\n",
      "Epoch [172/500], Train Loss: 0.0170\n",
      "Epoch [172/500], Test Loss: 0.0183\n",
      "Epoch [173/500], Train Loss: 0.0170\n",
      "Epoch [173/500], Test Loss: 0.0183\n",
      "Epoch [174/500], Train Loss: 0.0169\n",
      "Epoch [174/500], Test Loss: 0.0183\n",
      "Epoch [175/500], Train Loss: 0.0170\n",
      "Epoch [175/500], Test Loss: 0.0183\n",
      "Epoch [176/500], Train Loss: 0.0169\n",
      "Epoch [176/500], Test Loss: 0.0183\n",
      "Epoch [177/500], Train Loss: 0.0170\n",
      "Epoch [177/500], Test Loss: 0.0183\n",
      "Epoch [178/500], Train Loss: 0.0170\n",
      "Epoch [178/500], Test Loss: 0.0183\n",
      "Epoch [179/500], Train Loss: 0.0169\n",
      "Epoch [179/500], Test Loss: 0.0183\n",
      "Epoch [180/500], Train Loss: 0.0169\n",
      "Epoch [180/500], Test Loss: 0.0183\n",
      "Epoch [181/500], Train Loss: 0.0169\n",
      "Epoch [181/500], Test Loss: 0.0183\n",
      "Epoch [182/500], Train Loss: 0.0169\n",
      "Epoch [182/500], Test Loss: 0.0183\n",
      "Epoch [183/500], Train Loss: 0.0169\n",
      "Epoch [183/500], Test Loss: 0.0182\n",
      "Epoch [184/500], Train Loss: 0.0169\n",
      "Epoch [184/500], Test Loss: 0.0183\n",
      "Epoch [185/500], Train Loss: 0.0169\n",
      "Epoch [185/500], Test Loss: 0.0182\n",
      "Epoch [186/500], Train Loss: 0.0169\n",
      "Epoch [186/500], Test Loss: 0.0182\n",
      "Epoch [187/500], Train Loss: 0.0169\n",
      "Epoch [187/500], Test Loss: 0.0182\n",
      "Epoch [188/500], Train Loss: 0.0169\n",
      "Epoch [188/500], Test Loss: 0.0182\n",
      "Epoch [189/500], Train Loss: 0.0169\n",
      "Epoch [189/500], Test Loss: 0.0182\n",
      "Epoch [190/500], Train Loss: 0.0169\n",
      "Epoch [190/500], Test Loss: 0.0182\n",
      "Epoch [191/500], Train Loss: 0.0169\n",
      "Epoch [191/500], Test Loss: 0.0182\n",
      "Epoch [192/500], Train Loss: 0.0169\n",
      "Epoch [192/500], Test Loss: 0.0182\n",
      "Epoch [193/500], Train Loss: 0.0169\n",
      "Epoch [193/500], Test Loss: 0.0182\n",
      "Epoch [194/500], Train Loss: 0.0169\n",
      "Epoch [194/500], Test Loss: 0.0182\n",
      "Epoch [195/500], Train Loss: 0.0169\n",
      "Epoch [195/500], Test Loss: 0.0182\n",
      "Epoch [196/500], Train Loss: 0.0169\n",
      "Epoch [196/500], Test Loss: 0.0182\n",
      "Epoch [197/500], Train Loss: 0.0169\n",
      "Epoch [197/500], Test Loss: 0.0182\n",
      "Epoch [198/500], Train Loss: 0.0169\n",
      "Epoch [198/500], Test Loss: 0.0182\n",
      "Epoch [199/500], Train Loss: 0.0169\n",
      "Epoch [199/500], Test Loss: 0.0182\n",
      "Epoch [200/500], Train Loss: 0.0169\n",
      "Epoch [200/500], Test Loss: 0.0182\n",
      "Epoch [201/500], Train Loss: 0.0169\n",
      "Epoch [201/500], Test Loss: 0.0182\n",
      "Epoch [202/500], Train Loss: 0.0169\n",
      "Epoch [202/500], Test Loss: 0.0182\n",
      "Epoch [203/500], Train Loss: 0.0169\n",
      "Epoch [203/500], Test Loss: 0.0182\n",
      "Epoch [204/500], Train Loss: 0.0168\n",
      "Epoch [204/500], Test Loss: 0.0182\n",
      "Epoch [205/500], Train Loss: 0.0168\n",
      "Epoch [205/500], Test Loss: 0.0182\n",
      "Epoch [206/500], Train Loss: 0.0169\n",
      "Epoch [206/500], Test Loss: 0.0181\n",
      "Epoch [207/500], Train Loss: 0.0169\n",
      "Epoch [207/500], Test Loss: 0.0181\n",
      "Epoch [208/500], Train Loss: 0.0169\n",
      "Epoch [208/500], Test Loss: 0.0181\n",
      "Epoch [209/500], Train Loss: 0.0169\n",
      "Epoch [209/500], Test Loss: 0.0181\n",
      "Epoch [210/500], Train Loss: 0.0168\n",
      "Epoch [210/500], Test Loss: 0.0182\n",
      "Epoch [211/500], Train Loss: 0.0168\n",
      "Epoch [211/500], Test Loss: 0.0181\n",
      "Epoch [212/500], Train Loss: 0.0169\n",
      "Epoch [212/500], Test Loss: 0.0182\n",
      "Epoch [213/500], Train Loss: 0.0168\n",
      "Epoch [213/500], Test Loss: 0.0181\n",
      "Epoch [214/500], Train Loss: 0.0168\n",
      "Epoch [214/500], Test Loss: 0.0181\n",
      "Epoch [215/500], Train Loss: 0.0168\n",
      "Epoch [215/500], Test Loss: 0.0181\n",
      "Epoch [216/500], Train Loss: 0.0168\n",
      "Epoch [216/500], Test Loss: 0.0181\n",
      "Epoch [217/500], Train Loss: 0.0168\n",
      "Epoch [217/500], Test Loss: 0.0181\n",
      "Epoch [218/500], Train Loss: 0.0169\n",
      "Epoch [218/500], Test Loss: 0.0181\n",
      "Epoch [219/500], Train Loss: 0.0168\n",
      "Epoch [219/500], Test Loss: 0.0181\n",
      "Epoch [220/500], Train Loss: 0.0168\n",
      "Epoch [220/500], Test Loss: 0.0181\n",
      "Epoch [221/500], Train Loss: 0.0168\n",
      "Epoch [221/500], Test Loss: 0.0181\n",
      "Epoch [222/500], Train Loss: 0.0168\n",
      "Epoch [222/500], Test Loss: 0.0181\n",
      "Epoch [223/500], Train Loss: 0.0168\n",
      "Epoch [223/500], Test Loss: 0.0181\n",
      "Epoch [224/500], Train Loss: 0.0168\n",
      "Epoch [224/500], Test Loss: 0.0181\n",
      "Epoch [225/500], Train Loss: 0.0168\n",
      "Epoch [225/500], Test Loss: 0.0181\n",
      "Epoch [226/500], Train Loss: 0.0168\n",
      "Epoch [226/500], Test Loss: 0.0181\n",
      "Epoch [227/500], Train Loss: 0.0168\n",
      "Epoch [227/500], Test Loss: 0.0181\n",
      "Epoch [228/500], Train Loss: 0.0168\n",
      "Epoch [228/500], Test Loss: 0.0181\n",
      "Epoch [229/500], Train Loss: 0.0168\n",
      "Epoch [229/500], Test Loss: 0.0181\n",
      "Epoch [230/500], Train Loss: 0.0168\n",
      "Epoch [230/500], Test Loss: 0.0181\n",
      "Epoch [231/500], Train Loss: 0.0167\n",
      "Epoch [231/500], Test Loss: 0.0181\n",
      "Epoch [232/500], Train Loss: 0.0168\n",
      "Epoch [232/500], Test Loss: 0.0181\n",
      "Epoch [233/500], Train Loss: 0.0167\n",
      "Epoch [233/500], Test Loss: 0.0181\n",
      "Epoch [234/500], Train Loss: 0.0168\n",
      "Epoch [234/500], Test Loss: 0.0181\n",
      "Epoch [235/500], Train Loss: 0.0168\n",
      "Epoch [235/500], Test Loss: 0.0181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [236/500], Train Loss: 0.0168\n",
      "Epoch [236/500], Test Loss: 0.0181\n",
      "Epoch [237/500], Train Loss: 0.0167\n",
      "Epoch [237/500], Test Loss: 0.0181\n",
      "Epoch [238/500], Train Loss: 0.0168\n",
      "Epoch [238/500], Test Loss: 0.0181\n",
      "Epoch [239/500], Train Loss: 0.0168\n",
      "Epoch [239/500], Test Loss: 0.0180\n",
      "Epoch [240/500], Train Loss: 0.0167\n",
      "Epoch [240/500], Test Loss: 0.0181\n",
      "Epoch [241/500], Train Loss: 0.0167\n",
      "Epoch [241/500], Test Loss: 0.0181\n",
      "Epoch [242/500], Train Loss: 0.0167\n",
      "Epoch [242/500], Test Loss: 0.0181\n",
      "Epoch [243/500], Train Loss: 0.0168\n",
      "Epoch [243/500], Test Loss: 0.0181\n",
      "Epoch [244/500], Train Loss: 0.0167\n",
      "Epoch [244/500], Test Loss: 0.0181\n",
      "Epoch [245/500], Train Loss: 0.0167\n",
      "Epoch [245/500], Test Loss: 0.0181\n",
      "Epoch [246/500], Train Loss: 0.0167\n",
      "Epoch [246/500], Test Loss: 0.0181\n",
      "Epoch [247/500], Train Loss: 0.0167\n",
      "Epoch [247/500], Test Loss: 0.0180\n",
      "Epoch [248/500], Train Loss: 0.0167\n",
      "Epoch [248/500], Test Loss: 0.0181\n",
      "Epoch [249/500], Train Loss: 0.0167\n",
      "Epoch [249/500], Test Loss: 0.0181\n",
      "Epoch [250/500], Train Loss: 0.0167\n",
      "Epoch [250/500], Test Loss: 0.0180\n",
      "Epoch [251/500], Train Loss: 0.0167\n",
      "Epoch [251/500], Test Loss: 0.0180\n",
      "Epoch [252/500], Train Loss: 0.0168\n",
      "Epoch [252/500], Test Loss: 0.0180\n",
      "Epoch [253/500], Train Loss: 0.0167\n",
      "Epoch [253/500], Test Loss: 0.0180\n",
      "Epoch [254/500], Train Loss: 0.0167\n",
      "Epoch [254/500], Test Loss: 0.0180\n",
      "Epoch [255/500], Train Loss: 0.0167\n",
      "Epoch [255/500], Test Loss: 0.0181\n",
      "Epoch [256/500], Train Loss: 0.0167\n",
      "Epoch [256/500], Test Loss: 0.0180\n",
      "Epoch [257/500], Train Loss: 0.0167\n",
      "Epoch [257/500], Test Loss: 0.0181\n",
      "Epoch [258/500], Train Loss: 0.0167\n",
      "Epoch [258/500], Test Loss: 0.0180\n",
      "Epoch [259/500], Train Loss: 0.0167\n",
      "Epoch [259/500], Test Loss: 0.0180\n",
      "Epoch [260/500], Train Loss: 0.0167\n",
      "Epoch [260/500], Test Loss: 0.0180\n",
      "Epoch [261/500], Train Loss: 0.0167\n",
      "Epoch [261/500], Test Loss: 0.0180\n",
      "Epoch [262/500], Train Loss: 0.0167\n",
      "Epoch [262/500], Test Loss: 0.0180\n",
      "Epoch [263/500], Train Loss: 0.0167\n",
      "Epoch [263/500], Test Loss: 0.0180\n",
      "Epoch [264/500], Train Loss: 0.0167\n",
      "Epoch [264/500], Test Loss: 0.0180\n",
      "Epoch [265/500], Train Loss: 0.0167\n",
      "Epoch [265/500], Test Loss: 0.0180\n",
      "Epoch [266/500], Train Loss: 0.0167\n",
      "Epoch [266/500], Test Loss: 0.0180\n",
      "Epoch [267/500], Train Loss: 0.0167\n",
      "Epoch [267/500], Test Loss: 0.0180\n",
      "Epoch [268/500], Train Loss: 0.0167\n",
      "Epoch [268/500], Test Loss: 0.0180\n",
      "Epoch [269/500], Train Loss: 0.0167\n",
      "Epoch [269/500], Test Loss: 0.0180\n",
      "Epoch [270/500], Train Loss: 0.0167\n",
      "Epoch [270/500], Test Loss: 0.0180\n",
      "Epoch [271/500], Train Loss: 0.0167\n",
      "Epoch [271/500], Test Loss: 0.0180\n",
      "Epoch [272/500], Train Loss: 0.0167\n",
      "Epoch [272/500], Test Loss: 0.0180\n",
      "Epoch [273/500], Train Loss: 0.0167\n",
      "Epoch [273/500], Test Loss: 0.0180\n",
      "Epoch [274/500], Train Loss: 0.0167\n",
      "Epoch [274/500], Test Loss: 0.0180\n",
      "Epoch [275/500], Train Loss: 0.0167\n",
      "Epoch [275/500], Test Loss: 0.0180\n",
      "Epoch [276/500], Train Loss: 0.0167\n",
      "Epoch [276/500], Test Loss: 0.0180\n",
      "Epoch [277/500], Train Loss: 0.0167\n",
      "Epoch [277/500], Test Loss: 0.0180\n",
      "Epoch [278/500], Train Loss: 0.0167\n",
      "Epoch [278/500], Test Loss: 0.0180\n",
      "Epoch [279/500], Train Loss: 0.0167\n",
      "Epoch [279/500], Test Loss: 0.0180\n",
      "Epoch [280/500], Train Loss: 0.0167\n",
      "Epoch [280/500], Test Loss: 0.0180\n",
      "Epoch [281/500], Train Loss: 0.0166\n",
      "Epoch [281/500], Test Loss: 0.0180\n",
      "Epoch [282/500], Train Loss: 0.0166\n",
      "Epoch [282/500], Test Loss: 0.0180\n",
      "Epoch [283/500], Train Loss: 0.0166\n",
      "Epoch [283/500], Test Loss: 0.0180\n",
      "Epoch [284/500], Train Loss: 0.0166\n",
      "Epoch [284/500], Test Loss: 0.0180\n",
      "Epoch [285/500], Train Loss: 0.0166\n",
      "Epoch [285/500], Test Loss: 0.0180\n",
      "Epoch [286/500], Train Loss: 0.0166\n",
      "Epoch [286/500], Test Loss: 0.0180\n",
      "Epoch [287/500], Train Loss: 0.0167\n",
      "Epoch [287/500], Test Loss: 0.0180\n",
      "Epoch [288/500], Train Loss: 0.0167\n",
      "Epoch [288/500], Test Loss: 0.0180\n",
      "Epoch [289/500], Train Loss: 0.0166\n",
      "Epoch [289/500], Test Loss: 0.0180\n",
      "Epoch [290/500], Train Loss: 0.0166\n",
      "Epoch [290/500], Test Loss: 0.0180\n",
      "Epoch [291/500], Train Loss: 0.0166\n",
      "Epoch [291/500], Test Loss: 0.0180\n",
      "Epoch [292/500], Train Loss: 0.0166\n",
      "Epoch [292/500], Test Loss: 0.0180\n",
      "Epoch [293/500], Train Loss: 0.0166\n",
      "Epoch [293/500], Test Loss: 0.0180\n",
      "Epoch [294/500], Train Loss: 0.0166\n",
      "Epoch [294/500], Test Loss: 0.0180\n",
      "Epoch [295/500], Train Loss: 0.0166\n",
      "Epoch [295/500], Test Loss: 0.0180\n",
      "Epoch [296/500], Train Loss: 0.0166\n",
      "Epoch [296/500], Test Loss: 0.0180\n",
      "Epoch [297/500], Train Loss: 0.0166\n",
      "Epoch [297/500], Test Loss: 0.0180\n",
      "Epoch [298/500], Train Loss: 0.0166\n",
      "Epoch [298/500], Test Loss: 0.0180\n",
      "Epoch [299/500], Train Loss: 0.0166\n",
      "Epoch [299/500], Test Loss: 0.0180\n",
      "Epoch [300/500], Train Loss: 0.0166\n",
      "Epoch [300/500], Test Loss: 0.0180\n",
      "Epoch [301/500], Train Loss: 0.0166\n",
      "Epoch [301/500], Test Loss: 0.0180\n",
      "Epoch [302/500], Train Loss: 0.0166\n",
      "Epoch [302/500], Test Loss: 0.0179\n",
      "Epoch [303/500], Train Loss: 0.0166\n",
      "Epoch [303/500], Test Loss: 0.0179\n",
      "Epoch [304/500], Train Loss: 0.0166\n",
      "Epoch [304/500], Test Loss: 0.0180\n",
      "Epoch [305/500], Train Loss: 0.0166\n",
      "Epoch [305/500], Test Loss: 0.0179\n",
      "Epoch [306/500], Train Loss: 0.0166\n",
      "Epoch [306/500], Test Loss: 0.0179\n",
      "Epoch [307/500], Train Loss: 0.0166\n",
      "Epoch [307/500], Test Loss: 0.0179\n",
      "Epoch [308/500], Train Loss: 0.0166\n",
      "Epoch [308/500], Test Loss: 0.0179\n",
      "Epoch [309/500], Train Loss: 0.0166\n",
      "Epoch [309/500], Test Loss: 0.0179\n",
      "Epoch [310/500], Train Loss: 0.0166\n",
      "Epoch [310/500], Test Loss: 0.0179\n",
      "Epoch [311/500], Train Loss: 0.0166\n",
      "Epoch [311/500], Test Loss: 0.0180\n",
      "Epoch [312/500], Train Loss: 0.0166\n",
      "Epoch [312/500], Test Loss: 0.0179\n",
      "Epoch [313/500], Train Loss: 0.0166\n",
      "Epoch [313/500], Test Loss: 0.0180\n",
      "Epoch [314/500], Train Loss: 0.0166\n",
      "Epoch [314/500], Test Loss: 0.0179\n",
      "Epoch [315/500], Train Loss: 0.0166\n",
      "Epoch [315/500], Test Loss: 0.0179\n",
      "Epoch [316/500], Train Loss: 0.0166\n",
      "Epoch [316/500], Test Loss: 0.0180\n",
      "Epoch [317/500], Train Loss: 0.0166\n",
      "Epoch [317/500], Test Loss: 0.0179\n",
      "Epoch [318/500], Train Loss: 0.0166\n",
      "Epoch [318/500], Test Loss: 0.0179\n",
      "Epoch [319/500], Train Loss: 0.0166\n",
      "Epoch [319/500], Test Loss: 0.0179\n",
      "Epoch [320/500], Train Loss: 0.0166\n",
      "Epoch [320/500], Test Loss: 0.0179\n",
      "Epoch [321/500], Train Loss: 0.0166\n",
      "Epoch [321/500], Test Loss: 0.0179\n",
      "Epoch [322/500], Train Loss: 0.0166\n",
      "Epoch [322/500], Test Loss: 0.0179\n",
      "Epoch [323/500], Train Loss: 0.0166\n",
      "Epoch [323/500], Test Loss: 0.0179\n",
      "Epoch [324/500], Train Loss: 0.0166\n",
      "Epoch [324/500], Test Loss: 0.0179\n",
      "Epoch [325/500], Train Loss: 0.0166\n",
      "Epoch [325/500], Test Loss: 0.0179\n",
      "Epoch [326/500], Train Loss: 0.0166\n",
      "Epoch [326/500], Test Loss: 0.0179\n",
      "Epoch [327/500], Train Loss: 0.0166\n",
      "Epoch [327/500], Test Loss: 0.0179\n",
      "Epoch [328/500], Train Loss: 0.0166\n",
      "Epoch [328/500], Test Loss: 0.0179\n",
      "Epoch [329/500], Train Loss: 0.0166\n",
      "Epoch [329/500], Test Loss: 0.0179\n",
      "Epoch [330/500], Train Loss: 0.0166\n",
      "Epoch [330/500], Test Loss: 0.0179\n",
      "Epoch [331/500], Train Loss: 0.0166\n",
      "Epoch [331/500], Test Loss: 0.0179\n",
      "Epoch [332/500], Train Loss: 0.0166\n",
      "Epoch [332/500], Test Loss: 0.0179\n",
      "Epoch [333/500], Train Loss: 0.0166\n",
      "Epoch [333/500], Test Loss: 0.0179\n",
      "Epoch [334/500], Train Loss: 0.0166\n",
      "Epoch [334/500], Test Loss: 0.0179\n",
      "Epoch [335/500], Train Loss: 0.0166\n",
      "Epoch [335/500], Test Loss: 0.0179\n",
      "Epoch [336/500], Train Loss: 0.0165\n",
      "Epoch [336/500], Test Loss: 0.0179\n",
      "Epoch [337/500], Train Loss: 0.0166\n",
      "Epoch [337/500], Test Loss: 0.0179\n",
      "Epoch [338/500], Train Loss: 0.0165\n",
      "Epoch [338/500], Test Loss: 0.0179\n",
      "Epoch [339/500], Train Loss: 0.0165\n",
      "Epoch [339/500], Test Loss: 0.0179\n",
      "Epoch [340/500], Train Loss: 0.0165\n",
      "Epoch [340/500], Test Loss: 0.0179\n",
      "Epoch [341/500], Train Loss: 0.0165\n",
      "Epoch [341/500], Test Loss: 0.0179\n",
      "Epoch [342/500], Train Loss: 0.0165\n",
      "Epoch [342/500], Test Loss: 0.0179\n",
      "Epoch [343/500], Train Loss: 0.0165\n",
      "Epoch [343/500], Test Loss: 0.0179\n",
      "Epoch [344/500], Train Loss: 0.0165\n",
      "Epoch [344/500], Test Loss: 0.0179\n",
      "Epoch [345/500], Train Loss: 0.0165\n",
      "Epoch [345/500], Test Loss: 0.0179\n",
      "Epoch [346/500], Train Loss: 0.0165\n",
      "Epoch [346/500], Test Loss: 0.0179\n",
      "Epoch [347/500], Train Loss: 0.0165\n",
      "Epoch [347/500], Test Loss: 0.0179\n",
      "Epoch [348/500], Train Loss: 0.0165\n",
      "Epoch [348/500], Test Loss: 0.0179\n",
      "Epoch [349/500], Train Loss: 0.0165\n",
      "Epoch [349/500], Test Loss: 0.0179\n",
      "Epoch [350/500], Train Loss: 0.0165\n",
      "Epoch [350/500], Test Loss: 0.0179\n",
      "Epoch [351/500], Train Loss: 0.0165\n",
      "Epoch [351/500], Test Loss: 0.0179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [352/500], Train Loss: 0.0165\n",
      "Epoch [352/500], Test Loss: 0.0179\n",
      "Epoch [353/500], Train Loss: 0.0165\n",
      "Epoch [353/500], Test Loss: 0.0179\n",
      "Epoch [354/500], Train Loss: 0.0165\n",
      "Epoch [354/500], Test Loss: 0.0178\n",
      "Epoch [355/500], Train Loss: 0.0165\n",
      "Epoch [355/500], Test Loss: 0.0178\n",
      "Epoch [356/500], Train Loss: 0.0165\n",
      "Epoch [356/500], Test Loss: 0.0179\n",
      "Epoch [357/500], Train Loss: 0.0165\n",
      "Epoch [357/500], Test Loss: 0.0179\n",
      "Epoch [358/500], Train Loss: 0.0165\n",
      "Epoch [358/500], Test Loss: 0.0178\n",
      "Epoch [359/500], Train Loss: 0.0165\n",
      "Epoch [359/500], Test Loss: 0.0179\n",
      "Epoch [360/500], Train Loss: 0.0165\n",
      "Epoch [360/500], Test Loss: 0.0178\n",
      "Epoch [361/500], Train Loss: 0.0165\n",
      "Epoch [361/500], Test Loss: 0.0179\n",
      "Epoch [362/500], Train Loss: 0.0165\n",
      "Epoch [362/500], Test Loss: 0.0179\n",
      "Epoch [363/500], Train Loss: 0.0165\n",
      "Epoch [363/500], Test Loss: 0.0178\n",
      "Epoch [364/500], Train Loss: 0.0165\n",
      "Epoch [364/500], Test Loss: 0.0178\n",
      "Epoch [365/500], Train Loss: 0.0165\n",
      "Epoch [365/500], Test Loss: 0.0178\n",
      "Epoch [366/500], Train Loss: 0.0165\n",
      "Epoch [366/500], Test Loss: 0.0178\n",
      "Epoch [367/500], Train Loss: 0.0165\n",
      "Epoch [367/500], Test Loss: 0.0179\n",
      "Epoch [368/500], Train Loss: 0.0165\n",
      "Epoch [368/500], Test Loss: 0.0178\n",
      "Epoch [369/500], Train Loss: 0.0165\n",
      "Epoch [369/500], Test Loss: 0.0178\n",
      "Epoch [370/500], Train Loss: 0.0165\n",
      "Epoch [370/500], Test Loss: 0.0178\n",
      "Epoch [371/500], Train Loss: 0.0165\n",
      "Epoch [371/500], Test Loss: 0.0178\n",
      "Epoch [372/500], Train Loss: 0.0165\n",
      "Epoch [372/500], Test Loss: 0.0178\n",
      "Epoch [373/500], Train Loss: 0.0165\n",
      "Epoch [373/500], Test Loss: 0.0178\n",
      "Epoch [374/500], Train Loss: 0.0165\n",
      "Epoch [374/500], Test Loss: 0.0178\n",
      "Epoch [375/500], Train Loss: 0.0165\n",
      "Epoch [375/500], Test Loss: 0.0178\n",
      "Epoch [376/500], Train Loss: 0.0165\n",
      "Epoch [376/500], Test Loss: 0.0178\n",
      "Epoch [377/500], Train Loss: 0.0165\n",
      "Epoch [377/500], Test Loss: 0.0178\n",
      "Epoch [378/500], Train Loss: 0.0165\n",
      "Epoch [378/500], Test Loss: 0.0178\n",
      "Epoch [379/500], Train Loss: 0.0165\n",
      "Epoch [379/500], Test Loss: 0.0178\n",
      "Epoch [380/500], Train Loss: 0.0165\n",
      "Epoch [380/500], Test Loss: 0.0178\n",
      "Epoch [381/500], Train Loss: 0.0165\n",
      "Epoch [381/500], Test Loss: 0.0178\n",
      "Epoch [382/500], Train Loss: 0.0165\n",
      "Epoch [382/500], Test Loss: 0.0178\n",
      "Epoch [383/500], Train Loss: 0.0165\n",
      "Epoch [383/500], Test Loss: 0.0178\n",
      "Epoch [384/500], Train Loss: 0.0165\n",
      "Epoch [384/500], Test Loss: 0.0178\n",
      "Epoch [385/500], Train Loss: 0.0165\n",
      "Epoch [385/500], Test Loss: 0.0178\n",
      "Epoch [386/500], Train Loss: 0.0165\n",
      "Epoch [386/500], Test Loss: 0.0178\n",
      "Epoch [387/500], Train Loss: 0.0165\n",
      "Epoch [387/500], Test Loss: 0.0178\n",
      "Epoch [388/500], Train Loss: 0.0165\n",
      "Epoch [388/500], Test Loss: 0.0178\n",
      "Epoch [389/500], Train Loss: 0.0165\n",
      "Epoch [389/500], Test Loss: 0.0178\n",
      "Epoch [390/500], Train Loss: 0.0165\n",
      "Epoch [390/500], Test Loss: 0.0178\n",
      "Epoch [391/500], Train Loss: 0.0165\n",
      "Epoch [391/500], Test Loss: 0.0178\n",
      "Epoch [392/500], Train Loss: 0.0165\n",
      "Epoch [392/500], Test Loss: 0.0178\n",
      "Epoch [393/500], Train Loss: 0.0165\n",
      "Epoch [393/500], Test Loss: 0.0178\n",
      "Epoch [394/500], Train Loss: 0.0165\n",
      "Epoch [394/500], Test Loss: 0.0178\n",
      "Epoch [395/500], Train Loss: 0.0165\n",
      "Epoch [395/500], Test Loss: 0.0178\n",
      "Epoch [396/500], Train Loss: 0.0165\n",
      "Epoch [396/500], Test Loss: 0.0178\n",
      "Epoch [397/500], Train Loss: 0.0165\n",
      "Epoch [397/500], Test Loss: 0.0178\n",
      "Epoch [398/500], Train Loss: 0.0165\n",
      "Epoch [398/500], Test Loss: 0.0178\n",
      "Epoch [399/500], Train Loss: 0.0165\n",
      "Epoch [399/500], Test Loss: 0.0178\n",
      "Epoch [400/500], Train Loss: 0.0165\n",
      "Epoch [400/500], Test Loss: 0.0178\n",
      "Epoch [401/500], Train Loss: 0.0165\n",
      "Epoch [401/500], Test Loss: 0.0178\n",
      "Epoch [402/500], Train Loss: 0.0165\n",
      "Epoch [402/500], Test Loss: 0.0178\n",
      "Epoch [403/500], Train Loss: 0.0165\n",
      "Epoch [403/500], Test Loss: 0.0178\n",
      "Epoch [404/500], Train Loss: 0.0165\n",
      "Epoch [404/500], Test Loss: 0.0178\n",
      "Epoch [405/500], Train Loss: 0.0164\n",
      "Epoch [405/500], Test Loss: 0.0178\n",
      "Epoch [406/500], Train Loss: 0.0165\n",
      "Epoch [406/500], Test Loss: 0.0178\n",
      "Epoch [407/500], Train Loss: 0.0164\n",
      "Epoch [407/500], Test Loss: 0.0178\n",
      "Epoch [408/500], Train Loss: 0.0164\n",
      "Epoch [408/500], Test Loss: 0.0178\n",
      "Epoch [409/500], Train Loss: 0.0164\n",
      "Epoch [409/500], Test Loss: 0.0178\n",
      "Epoch [410/500], Train Loss: 0.0165\n",
      "Epoch [410/500], Test Loss: 0.0178\n",
      "Epoch [411/500], Train Loss: 0.0165\n",
      "Epoch [411/500], Test Loss: 0.0178\n",
      "Epoch [412/500], Train Loss: 0.0165\n",
      "Epoch [412/500], Test Loss: 0.0178\n",
      "Epoch [413/500], Train Loss: 0.0164\n",
      "Epoch [413/500], Test Loss: 0.0178\n",
      "Epoch [414/500], Train Loss: 0.0165\n",
      "Epoch [414/500], Test Loss: 0.0178\n",
      "Epoch [415/500], Train Loss: 0.0165\n",
      "Epoch [415/500], Test Loss: 0.0178\n",
      "Epoch [416/500], Train Loss: 0.0164\n",
      "Epoch [416/500], Test Loss: 0.0178\n",
      "Epoch [417/500], Train Loss: 0.0165\n",
      "Epoch [417/500], Test Loss: 0.0178\n",
      "Epoch [418/500], Train Loss: 0.0164\n",
      "Epoch [418/500], Test Loss: 0.0178\n",
      "Epoch [419/500], Train Loss: 0.0164\n",
      "Epoch [419/500], Test Loss: 0.0178\n",
      "Epoch [420/500], Train Loss: 0.0164\n",
      "Epoch [420/500], Test Loss: 0.0178\n",
      "Epoch [421/500], Train Loss: 0.0164\n",
      "Epoch [421/500], Test Loss: 0.0178\n",
      "Epoch [422/500], Train Loss: 0.0165\n",
      "Epoch [422/500], Test Loss: 0.0178\n",
      "Epoch [423/500], Train Loss: 0.0165\n",
      "Epoch [423/500], Test Loss: 0.0178\n",
      "Epoch [424/500], Train Loss: 0.0165\n",
      "Epoch [424/500], Test Loss: 0.0178\n",
      "Epoch [425/500], Train Loss: 0.0164\n",
      "Epoch [425/500], Test Loss: 0.0178\n",
      "Epoch [426/500], Train Loss: 0.0164\n",
      "Epoch [426/500], Test Loss: 0.0178\n",
      "Epoch [427/500], Train Loss: 0.0164\n",
      "Epoch [427/500], Test Loss: 0.0178\n",
      "Epoch [428/500], Train Loss: 0.0164\n",
      "Epoch [428/500], Test Loss: 0.0178\n",
      "Epoch [429/500], Train Loss: 0.0164\n",
      "Epoch [429/500], Test Loss: 0.0178\n",
      "Epoch [430/500], Train Loss: 0.0164\n",
      "Epoch [430/500], Test Loss: 0.0177\n",
      "Epoch [431/500], Train Loss: 0.0164\n",
      "Epoch [431/500], Test Loss: 0.0178\n",
      "Epoch [432/500], Train Loss: 0.0164\n",
      "Epoch [432/500], Test Loss: 0.0178\n",
      "Epoch [433/500], Train Loss: 0.0164\n",
      "Epoch [433/500], Test Loss: 0.0178\n",
      "Epoch [434/500], Train Loss: 0.0164\n",
      "Epoch [434/500], Test Loss: 0.0178\n",
      "Epoch [435/500], Train Loss: 0.0164\n",
      "Epoch [435/500], Test Loss: 0.0178\n",
      "Epoch [436/500], Train Loss: 0.0164\n",
      "Epoch [436/500], Test Loss: 0.0178\n",
      "Epoch [437/500], Train Loss: 0.0164\n",
      "Epoch [437/500], Test Loss: 0.0178\n",
      "Epoch [438/500], Train Loss: 0.0164\n",
      "Epoch [438/500], Test Loss: 0.0178\n",
      "Epoch [439/500], Train Loss: 0.0164\n",
      "Epoch [439/500], Test Loss: 0.0178\n",
      "Epoch [440/500], Train Loss: 0.0164\n",
      "Epoch [440/500], Test Loss: 0.0177\n",
      "Epoch [441/500], Train Loss: 0.0164\n",
      "Epoch [441/500], Test Loss: 0.0178\n",
      "Epoch [442/500], Train Loss: 0.0164\n",
      "Epoch [442/500], Test Loss: 0.0178\n",
      "Epoch [443/500], Train Loss: 0.0164\n",
      "Epoch [443/500], Test Loss: 0.0177\n",
      "Epoch [444/500], Train Loss: 0.0164\n",
      "Epoch [444/500], Test Loss: 0.0178\n",
      "Epoch [445/500], Train Loss: 0.0164\n",
      "Epoch [445/500], Test Loss: 0.0178\n",
      "Epoch [446/500], Train Loss: 0.0164\n",
      "Epoch [446/500], Test Loss: 0.0178\n",
      "Epoch [447/500], Train Loss: 0.0164\n",
      "Epoch [447/500], Test Loss: 0.0178\n",
      "Epoch [448/500], Train Loss: 0.0164\n",
      "Epoch [448/500], Test Loss: 0.0178\n",
      "Epoch [449/500], Train Loss: 0.0164\n",
      "Epoch [449/500], Test Loss: 0.0177\n",
      "Epoch [450/500], Train Loss: 0.0164\n",
      "Epoch [450/500], Test Loss: 0.0177\n",
      "Epoch [451/500], Train Loss: 0.0164\n",
      "Epoch [451/500], Test Loss: 0.0178\n",
      "Epoch [452/500], Train Loss: 0.0164\n",
      "Epoch [452/500], Test Loss: 0.0178\n",
      "Epoch [453/500], Train Loss: 0.0164\n",
      "Epoch [453/500], Test Loss: 0.0178\n",
      "Epoch [454/500], Train Loss: 0.0164\n",
      "Epoch [454/500], Test Loss: 0.0178\n",
      "Epoch [455/500], Train Loss: 0.0164\n",
      "Epoch [455/500], Test Loss: 0.0178\n",
      "Epoch [456/500], Train Loss: 0.0164\n",
      "Epoch [456/500], Test Loss: 0.0177\n",
      "Epoch [457/500], Train Loss: 0.0164\n",
      "Epoch [457/500], Test Loss: 0.0177\n",
      "Epoch [458/500], Train Loss: 0.0164\n",
      "Epoch [458/500], Test Loss: 0.0178\n",
      "Epoch [459/500], Train Loss: 0.0164\n",
      "Epoch [459/500], Test Loss: 0.0178\n",
      "Epoch [460/500], Train Loss: 0.0164\n",
      "Epoch [460/500], Test Loss: 0.0177\n",
      "Epoch [461/500], Train Loss: 0.0164\n",
      "Epoch [461/500], Test Loss: 0.0177\n",
      "Epoch [462/500], Train Loss: 0.0164\n",
      "Epoch [462/500], Test Loss: 0.0177\n",
      "Epoch [463/500], Train Loss: 0.0164\n",
      "Epoch [463/500], Test Loss: 0.0177\n",
      "Epoch [464/500], Train Loss: 0.0164\n",
      "Epoch [464/500], Test Loss: 0.0177\n",
      "Epoch [465/500], Train Loss: 0.0164\n",
      "Epoch [465/500], Test Loss: 0.0177\n",
      "Epoch [466/500], Train Loss: 0.0164\n",
      "Epoch [466/500], Test Loss: 0.0177\n",
      "Epoch [467/500], Train Loss: 0.0164\n",
      "Epoch [467/500], Test Loss: 0.0177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [468/500], Train Loss: 0.0164\n",
      "Epoch [468/500], Test Loss: 0.0177\n",
      "Epoch [469/500], Train Loss: 0.0164\n",
      "Epoch [469/500], Test Loss: 0.0177\n",
      "Epoch [470/500], Train Loss: 0.0164\n",
      "Epoch [470/500], Test Loss: 0.0177\n",
      "Epoch [471/500], Train Loss: 0.0164\n",
      "Epoch [471/500], Test Loss: 0.0177\n",
      "Epoch [472/500], Train Loss: 0.0164\n",
      "Epoch [472/500], Test Loss: 0.0177\n",
      "Epoch [473/500], Train Loss: 0.0164\n",
      "Epoch [473/500], Test Loss: 0.0177\n",
      "Epoch [474/500], Train Loss: 0.0164\n",
      "Epoch [474/500], Test Loss: 0.0177\n",
      "Epoch [475/500], Train Loss: 0.0164\n",
      "Epoch [475/500], Test Loss: 0.0177\n",
      "Epoch [476/500], Train Loss: 0.0164\n",
      "Epoch [476/500], Test Loss: 0.0177\n",
      "Epoch [477/500], Train Loss: 0.0164\n",
      "Epoch [477/500], Test Loss: 0.0177\n",
      "Epoch [478/500], Train Loss: 0.0164\n",
      "Epoch [478/500], Test Loss: 0.0177\n",
      "Epoch [479/500], Train Loss: 0.0164\n",
      "Epoch [479/500], Test Loss: 0.0178\n",
      "Epoch [480/500], Train Loss: 0.0164\n",
      "Epoch [480/500], Test Loss: 0.0177\n",
      "Epoch [481/500], Train Loss: 0.0164\n",
      "Epoch [481/500], Test Loss: 0.0177\n",
      "Epoch [482/500], Train Loss: 0.0164\n",
      "Epoch [482/500], Test Loss: 0.0177\n",
      "Epoch [483/500], Train Loss: 0.0164\n",
      "Epoch [483/500], Test Loss: 0.0177\n",
      "Epoch [484/500], Train Loss: 0.0164\n",
      "Epoch [484/500], Test Loss: 0.0177\n",
      "Epoch [485/500], Train Loss: 0.0164\n",
      "Epoch [485/500], Test Loss: 0.0177\n",
      "Epoch [486/500], Train Loss: 0.0164\n",
      "Epoch [486/500], Test Loss: 0.0177\n",
      "Epoch [487/500], Train Loss: 0.0164\n",
      "Epoch [487/500], Test Loss: 0.0177\n",
      "Epoch [488/500], Train Loss: 0.0164\n",
      "Epoch [488/500], Test Loss: 0.0177\n",
      "Epoch [489/500], Train Loss: 0.0164\n",
      "Epoch [489/500], Test Loss: 0.0177\n",
      "Epoch [490/500], Train Loss: 0.0164\n",
      "Epoch [490/500], Test Loss: 0.0177\n",
      "Epoch [491/500], Train Loss: 0.0164\n",
      "Epoch [491/500], Test Loss: 0.0177\n",
      "Epoch [492/500], Train Loss: 0.0164\n",
      "Epoch [492/500], Test Loss: 0.0177\n",
      "Epoch [493/500], Train Loss: 0.0164\n",
      "Epoch [493/500], Test Loss: 0.0177\n",
      "Epoch [494/500], Train Loss: 0.0164\n",
      "Epoch [494/500], Test Loss: 0.0177\n",
      "Epoch [495/500], Train Loss: 0.0164\n",
      "Epoch [495/500], Test Loss: 0.0177\n",
      "Epoch [496/500], Train Loss: 0.0164\n",
      "Epoch [496/500], Test Loss: 0.0177\n",
      "Epoch [497/500], Train Loss: 0.0164\n",
      "Epoch [497/500], Test Loss: 0.0177\n",
      "Epoch [498/500], Train Loss: 0.0164\n",
      "Epoch [498/500], Test Loss: 0.0177\n",
      "Epoch [499/500], Train Loss: 0.0164\n",
      "Epoch [499/500], Test Loss: 0.0177\n",
      "Epoch [500/500], Train Loss: 0.0164\n",
      "Epoch [500/500], Test Loss: 0.0177\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "# Training loop\n",
    "num_epochs = 500\n",
    "for epoch in range(num_epochs):\n",
    "    for (img1,img2) in train_loader:\n",
    "        img1 = img1.to(device)\n",
    "        img2 = img2.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        encoded_x1, encoded_x2, decoded_x1, decoded_x2 = model(img1, img2)\n",
    "        # Calculate the modified contrastive loss using MSE loss\n",
    "        loss_contrastive = contrastive_loss(encoded_x1, encoded_x2)\n",
    "        \n",
    "        # Calculate the reconstruction loss\n",
    "        loss_reconstruction1 = reconstruction_loss(decoded_x1, img1)\n",
    "        loss_reconstruction2 = reconstruction_loss(decoded_x2, img2)\n",
    "\n",
    "        # Combine the losses with appropriate weights\n",
    "        loss = loss_contrastive + 0.5 * (loss_reconstruction1 + loss_reconstruction2)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for (img1,img2) in test_loader:\n",
    "            img1 = img1.to(device)\n",
    "            img2 = img2.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            encoded_x1, encoded_x2, decoded_x1, decoded_x2 = model(img1, img2)\n",
    "            # Calculate the modified contrastive loss using MSE loss\n",
    "            loss_contrastive = contrastive_loss(encoded_x1, encoded_x2)\n",
    "\n",
    "            # Calculate the reconstruction loss\n",
    "            loss_reconstruction1 = reconstruction_loss(decoded_x1, img1)\n",
    "            loss_reconstruction2 = reconstruction_loss(decoded_x2, img2)\n",
    "\n",
    "            # Combine the losses with appropriate weights\n",
    "            loss_test = loss_contrastive + 0.5 * (loss_reconstruction1 + loss_reconstruction2)            \n",
    "            \n",
    "            \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item():.4f}')\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Test Loss: {loss_test.item():.4f}')\n",
    "    train_losses.append(loss.item())\n",
    "    test_losses.append(loss_test.item())\n",
    "    # Save the trained model\n",
    "    path = 'SiameseCAE_Model(64)_MapLearning/epoch_{i}.pth'.format(i=epoch+1)\n",
    "    torch.save(model.state_dict(), path)\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a14a86f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SiameseCAE(\n",
       "  (cae): CAE(\n",
       "    (encoder): Encoder(\n",
       "      (encoder_): Sequential(\n",
       "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (6): ReLU()\n",
       "        (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (10): ReLU()\n",
       "        (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "    (decoder): Decoder(\n",
       "      (decoder_): Sequential(\n",
       "        (0): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU()\n",
       "        (6): ConvTranspose2d(32, 3, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (7): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "state = torch.load(\"E:\\PG_HP_FINAL\\CAE Learn Map difference\\SiameseCAE_Model(64)_MapLearning\\epoch_500.pth\")\n",
    "model = SiameseCAE()\n",
    "model.load_state_dict(state)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9ad3e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "unloader = transforms.ToPILImage()\n",
    "def tensor_to_PIL(tensor):\n",
    "    image = tensor.cpu().clone()\n",
    "    image = image.squeeze(0)\n",
    "    image = unloader(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d393a675",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for idx, (inputs,targets) in enumerate(train_loader):\n",
    "        encoded_x1, encoded_x2, decoded_x1, decoded_x2 = model(inputs, targets)\n",
    "        for index,xxx in enumerate(decoded_x1):\n",
    "            original_x = inputs[index]\n",
    "            reconstr_x = xxx\n",
    "            target_x = decoded_x2[index]\n",
    "\n",
    "            image1 = tensor_to_PIL(original_x)\n",
    "            image2 = tensor_to_PIL(reconstr_x)\n",
    "            image3 =  tensor_to_PIL(target_x)\n",
    "            path1 =  'img(ori)MSE/patch_{num}_original(train).png'.format(num=idx*256+index)\n",
    "            path2 =  'img(recon)MSE/patch_{num}_reconstruct(train).png'.format(num=idx*256+index)\n",
    "            path3 =  'img(target)MSE/patch_{num}_target(train).png'.format(num=idx*256+index)\n",
    "            \n",
    "            image1.save(path1)\n",
    "            image2.save(path2)  \n",
    "            image3.save(path3)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd41e736",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ML] *",
   "language": "python",
   "name": "conda-env-ML-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
