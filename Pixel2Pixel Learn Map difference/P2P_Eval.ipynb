{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7004e25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "from patchify import patchify\n",
    "import PIL\n",
    "from PIL import Image\n",
    "PIL.Image.MAX_IMAGE_PIXELS = 933120000\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "%matplotlib inline\n",
    "import torchvision\n",
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f27cce9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchDataset(Dataset):\n",
    "    def __init__(self,root,target, train=True, transforms=None):\n",
    "        super(PatchDataset, self).__init__()\n",
    "\n",
    "        self.image_path = [os.path.join(root, x) for x in os.listdir(root)]      \n",
    "        self.ref_path = [os.path.join(target,x) for x in os.listdir(target)]\n",
    "        \n",
    "        if transform is not None:\n",
    "            self.transform = transform\n",
    "\n",
    "        if train:\n",
    "            self.images = self.image_path[: int(.8 * len(self.image_path))]\n",
    "            self.ref = self.ref_path[: int(.8 * len(self.image_path))]\n",
    "        else:\n",
    "            self.images = self.image_path[int(.8 * len(self.image_path)):]\n",
    "            self.ref = self.ref_path[int(.8 * len(self.image_path)):]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.transform(self.images[item]),self.transform(self.ref[item])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cf4cf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    lambda x: Image.open(x).convert('RGB'),\n",
    "    transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a6b73f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetGenerator(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNetGenerator, self).__init__()\n",
    "\n",
    "        # Encoding layers\n",
    "        self.encoder = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(in_channels, 64, kernel_size=4, stride=2, padding=1),\n",
    "                nn.LeakyReLU(0.2, inplace=True)\n",
    "            ),\n",
    "            self._conv_block(64, 128),\n",
    "            self._conv_block(128, 256),\n",
    "            self._conv_block(256, 512)\n",
    "        ])\n",
    "\n",
    "        # Decoding layers\n",
    "        self.decoder = nn.ModuleList([\n",
    "            self._deconv_block(512, 256),\n",
    "            self._deconv_block(512, 128),\n",
    "            self._deconv_block(256, 64),\n",
    "            nn.ConvTranspose2d(128, out_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        skips = []\n",
    "        for layer in self.encoder:\n",
    "            x = layer(x)\n",
    "            skips.append(x)\n",
    "\n",
    "        skips = list(reversed(skips[:-1]))\n",
    "        for idx, (skip, layer) in enumerate(zip(skips, self.decoder[:-2])):\n",
    "            x = layer(x)\n",
    "            x = torch.cat((x, skip), dim=1)\n",
    "\n",
    "        x = self.decoder[-2](x)\n",
    "        x = self.decoder[-1](x)\n",
    "        return x\n",
    "\n",
    "    def _conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "    def _deconv_block(self, in_channels, out_channels, dropout=0.0):\n",
    "        layers = [\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "        if dropout:\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        return nn.Sequential(*layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66955d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            self._conv_block(64, 128),\n",
    "            self._conv_block(128, 256),\n",
    "            self._conv_block(256, 512),\n",
    "            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def _conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37edd85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PatchDataset(\"img415_patches(64)\",\"img418_patches(64)\",train=True, transforms=transform)\n",
    "test_dataset = PatchDataset(\"img415_patches(64)\",\"img418_patches(64)\",train=False, transforms=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30fa16ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 100\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "lambda_L1 = 100\n",
    "best_loss = 1e9\n",
    "# Initialize models\n",
    "generator = UNetGenerator(3, 3).to(device)\n",
    "discriminator = Discriminator(6).to(device)\n",
    "\n",
    "# Define loss functions and optimizers\n",
    "criterion_GAN = nn.MSELoss()\n",
    "criterion_L1 = nn.L1Loss()\n",
    "criterion_GAN = torch.nn.HuberLoss\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(beta1, 0.999))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c79fb90",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'HuberLoss' and 'HuberLoss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m loss_D_real \u001b[38;5;241m=\u001b[39m criterion_GAN(real_preds, real_targets)\n\u001b[0;32m     19\u001b[0m loss_D_fake \u001b[38;5;241m=\u001b[39m criterion_GAN(fake_preds, fake_targets)\n\u001b[1;32m---> 21\u001b[0m loss_D \u001b[38;5;241m=\u001b[39m (\u001b[43mloss_D_real\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss_D_fake\u001b[49m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[0;32m     22\u001b[0m loss_D\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     23\u001b[0m optimizer_D\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'HuberLoss' and 'HuberLoss'"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i, (image_1910, image_1970) in enumerate(train_loader):\n",
    "        image_1910 = image_1910.to(device)\n",
    "        image_1970 = image_1970.to(device)\n",
    "\n",
    "        # Train Discriminator\n",
    "        optimizer_D.zero_grad()\n",
    "        fake_image_1970 = generator(image_1910)\n",
    "        real_pair = torch.cat((image_1910, image_1970), dim=1)\n",
    "        fake_pair = torch.cat((image_1910, fake_image_1970.detach()), dim=1)\n",
    "\n",
    "        real_preds = discriminator(real_pair)\n",
    "        fake_preds = discriminator(fake_pair)\n",
    "\n",
    "        real_targets = torch.ones_like(real_preds).to(device)\n",
    "        fake_targets = torch.zeros_like(fake_preds).to(device)\n",
    "\n",
    "        loss_D_real = criterion_GAN(real_preds, real_targets)\n",
    "        loss_D_fake = criterion_GAN(fake_preds, fake_targets)\n",
    "\n",
    "        loss_D = (loss_D_real + loss_D_fake) * 0.5\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Train Generator\n",
    "        optimizer_G.zero_grad()\n",
    "        fake_preds = discriminator(fake_pair)\n",
    "\n",
    "        loss_G_GAN = criterion_GAN(fake_preds, real_targets)\n",
    "        loss_G_L1 = criterion_L1(fake_image_1970, image_1970) * lambda_L1\n",
    "\n",
    "        loss_G = loss_G_GAN + loss_G_L1\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "    \n",
    "    \n",
    "    path = 'model_P2P_SSR/generator_{i}.pth'.format(i=epoch)\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Loss D: {loss_D.item():.4f}, Loss G: {loss_G.item():.4f}')\n",
    "    torch.save(generator.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff1fcdeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss D: 0.3179, Loss G: 3.6422\n",
      "Epoch [2/100], Loss D: 0.2473, Loss G: 5.2486\n",
      "Epoch [3/100], Loss D: 0.1846, Loss G: 2.8642\n",
      "Epoch [4/100], Loss D: 0.1610, Loss G: 2.8926\n",
      "Epoch [5/100], Loss D: 0.0552, Loss G: 3.2287\n",
      "Epoch [6/100], Loss D: 0.2171, Loss G: 2.6547\n",
      "Epoch [7/100], Loss D: 0.1185, Loss G: 2.6292\n",
      "Epoch [8/100], Loss D: 0.0427, Loss G: 3.1130\n",
      "Epoch [9/100], Loss D: 0.0196, Loss G: 3.1847\n",
      "Epoch [10/100], Loss D: 0.0197, Loss G: 2.8270\n",
      "Epoch [11/100], Loss D: 0.0713, Loss G: 2.6242\n",
      "Epoch [12/100], Loss D: 0.0863, Loss G: 2.1308\n",
      "Epoch [13/100], Loss D: 0.0562, Loss G: 3.2406\n",
      "Epoch [14/100], Loss D: 0.0086, Loss G: 2.6976\n",
      "Epoch [15/100], Loss D: 0.0034, Loss G: 2.7385\n",
      "Epoch [16/100], Loss D: 0.0061, Loss G: 2.6612\n",
      "Epoch [17/100], Loss D: 0.2217, Loss G: 1.7533\n",
      "Epoch [18/100], Loss D: 0.0822, Loss G: 1.7919\n",
      "Epoch [19/100], Loss D: 0.0465, Loss G: 2.8218\n",
      "Epoch [20/100], Loss D: 0.0459, Loss G: 1.9137\n",
      "Epoch [21/100], Loss D: 0.0772, Loss G: 2.9829\n",
      "Epoch [22/100], Loss D: 0.0676, Loss G: 2.5276\n",
      "Epoch [23/100], Loss D: 0.0277, Loss G: 2.9693\n",
      "Epoch [24/100], Loss D: 0.0044, Loss G: 2.6348\n",
      "Epoch [25/100], Loss D: 0.0105, Loss G: 2.3300\n",
      "Epoch [26/100], Loss D: 0.3694, Loss G: 3.6338\n",
      "Epoch [27/100], Loss D: 0.0418, Loss G: 1.8515\n",
      "Epoch [28/100], Loss D: 0.0373, Loss G: 2.9962\n",
      "Epoch [29/100], Loss D: 0.3596, Loss G: 3.5489\n",
      "Epoch [30/100], Loss D: 0.3339, Loss G: 1.5973\n",
      "Epoch [31/100], Loss D: 0.0212, Loss G: 2.8150\n",
      "Epoch [32/100], Loss D: 0.4914, Loss G: 2.4906\n",
      "Epoch [33/100], Loss D: 0.0196, Loss G: 2.8877\n",
      "Epoch [34/100], Loss D: 0.0077, Loss G: 2.6574\n",
      "Epoch [35/100], Loss D: 0.3237, Loss G: 3.5553\n",
      "Epoch [36/100], Loss D: 0.1502, Loss G: 1.6654\n",
      "Epoch [37/100], Loss D: 0.4570, Loss G: 1.8932\n",
      "Epoch [38/100], Loss D: 0.0381, Loss G: 2.1064\n",
      "Epoch [39/100], Loss D: 0.1193, Loss G: 2.8745\n",
      "Epoch [40/100], Loss D: 0.0068, Loss G: 2.5075\n",
      "Epoch [41/100], Loss D: 0.3210, Loss G: 3.3993\n",
      "Epoch [42/100], Loss D: 0.0683, Loss G: 1.6829\n",
      "Epoch [43/100], Loss D: 0.1691, Loss G: 3.2499\n",
      "Epoch [44/100], Loss D: 0.0376, Loss G: 2.2300\n",
      "Epoch [45/100], Loss D: 0.1400, Loss G: 3.2138\n",
      "Epoch [46/100], Loss D: 0.0994, Loss G: 1.6186\n",
      "Epoch [47/100], Loss D: 0.3259, Loss G: 3.6307\n",
      "Epoch [48/100], Loss D: 0.0748, Loss G: 1.6503\n",
      "Epoch [49/100], Loss D: 0.0028, Loss G: 2.3714\n",
      "Epoch [50/100], Loss D: 0.0212, Loss G: 1.9081\n",
      "Epoch [51/100], Loss D: 0.0029, Loss G: 2.5563\n",
      "Epoch [52/100], Loss D: 0.0014, Loss G: 2.6341\n",
      "Epoch [53/100], Loss D: 0.4052, Loss G: 1.6627\n",
      "Epoch [54/100], Loss D: 0.0083, Loss G: 2.6372\n",
      "Epoch [55/100], Loss D: 0.0174, Loss G: 2.0746\n",
      "Epoch [56/100], Loss D: 0.0026, Loss G: 2.5313\n",
      "Epoch [57/100], Loss D: 0.0084, Loss G: 2.6773\n",
      "Epoch [58/100], Loss D: 0.0014, Loss G: 2.3277\n",
      "Epoch [59/100], Loss D: 0.0754, Loss G: 3.2713\n",
      "Epoch [60/100], Loss D: 0.0117, Loss G: 2.3941\n",
      "Epoch [61/100], Loss D: 0.0012, Loss G: 2.3814\n",
      "Epoch [62/100], Loss D: 0.0015, Loss G: 2.2314\n",
      "Epoch [63/100], Loss D: 0.0016, Loss G: 2.3738\n",
      "Epoch [64/100], Loss D: 0.0130, Loss G: 2.3195\n",
      "Epoch [65/100], Loss D: 0.0009, Loss G: 2.4915\n",
      "Epoch [66/100], Loss D: 0.0013, Loss G: 2.3942\n",
      "Epoch [67/100], Loss D: 0.0025, Loss G: 2.5754\n",
      "Epoch [68/100], Loss D: 0.0083, Loss G: 2.1624\n",
      "Epoch [69/100], Loss D: 0.0018, Loss G: 2.2695\n",
      "Epoch [70/100], Loss D: 0.0006, Loss G: 2.4726\n",
      "Epoch [71/100], Loss D: 0.0006, Loss G: 2.4100\n",
      "Epoch [72/100], Loss D: 0.0006, Loss G: 2.2850\n",
      "Epoch [73/100], Loss D: 0.0007, Loss G: 2.3885\n",
      "Epoch [74/100], Loss D: 0.0007, Loss G: 2.2975\n",
      "Epoch [75/100], Loss D: 0.0007, Loss G: 2.3936\n",
      "Epoch [76/100], Loss D: 0.0190, Loss G: 1.7200\n",
      "Epoch [77/100], Loss D: 0.0038, Loss G: 2.9019\n",
      "Epoch [78/100], Loss D: 0.0007, Loss G: 2.3355\n",
      "Epoch [79/100], Loss D: 0.0003, Loss G: 2.4136\n",
      "Epoch [80/100], Loss D: 0.0004, Loss G: 2.4758\n",
      "Epoch [81/100], Loss D: 0.0004, Loss G: 2.3526\n",
      "Epoch [82/100], Loss D: 0.2639, Loss G: 1.5508\n",
      "Epoch [83/100], Loss D: 0.0306, Loss G: 2.8536\n",
      "Epoch [84/100], Loss D: 0.0029, Loss G: 2.2417\n",
      "Epoch [85/100], Loss D: 0.0006, Loss G: 2.3256\n",
      "Epoch [86/100], Loss D: 0.0002, Loss G: 2.4301\n",
      "Epoch [87/100], Loss D: 0.0003, Loss G: 2.4826\n",
      "Epoch [88/100], Loss D: 0.0002, Loss G: 2.3783\n",
      "Epoch [89/100], Loss D: 0.0021, Loss G: 2.3758\n",
      "Epoch [90/100], Loss D: 0.0006, Loss G: 2.3111\n",
      "Epoch [91/100], Loss D: 0.0002, Loss G: 2.2688\n",
      "Epoch [92/100], Loss D: 0.0001, Loss G: 2.2510\n",
      "Epoch [93/100], Loss D: 0.0003, Loss G: 2.2427\n",
      "Epoch [94/100], Loss D: 0.0002, Loss G: 2.2965\n",
      "Epoch [95/100], Loss D: 0.0007, Loss G: 2.2280\n",
      "Epoch [96/100], Loss D: 0.0009, Loss G: 2.2522\n",
      "Epoch [97/100], Loss D: 0.0004, Loss G: 2.4048\n",
      "Epoch [98/100], Loss D: 0.0009, Loss G: 2.8322\n",
      "Epoch [99/100], Loss D: 0.0297, Loss G: 1.9002\n",
      "Epoch [100/100], Loss D: 0.0021, Loss G: 2.4196\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i, (image_1910, image_1970) in enumerate(train_loader):\n",
    "        image_1910 = image_1910.to(device)\n",
    "        image_1970 = image_1970.to(device)\n",
    "\n",
    "        # Train Discriminator\n",
    "        optimizer_D.zero_grad() \n",
    "        fake_image_1970 = generator(image_1910)\n",
    "        real_pair = torch.cat((image_1910, image_1970), dim=1)\n",
    "        fake_pair = torch.cat((image_1910, fake_image_1970.detach()), dim=1)\n",
    "\n",
    "        real_preds = discriminator(real_pair)\n",
    "        fake_preds = discriminator(fake_pair)\n",
    "\n",
    "        real_targets = torch.ones_like(real_preds).to(device)\n",
    "        fake_targets = torch.zeros_like(fake_preds).to(device)\n",
    "\n",
    "        loss_D_real = criterion_GAN(real_preds, real_targets)\n",
    "        loss_D_fake = criterion_GAN(fake_preds, fake_targets)\n",
    "\n",
    "        loss_D = (loss_D_real + loss_D_fake) * 0.5\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Train Generator\n",
    "        optimizer_G.zero_grad()\n",
    "        fake_preds = discriminator(fake_pair)\n",
    "\n",
    "        loss_G_GAN = criterion_GAN(fake_preds, real_targets)\n",
    "        loss_G_L1 = criterion_L1(fake_image_1970, image_1970) * lambda_L1\n",
    "\n",
    "        loss_G = loss_G_GAN + loss_G_L1\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "    \n",
    "    \n",
    "    path = 'model_P2P_SSR/generator_{i}.pth'.format(i=epoch)\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Loss D: {loss_D.item():.4f}, Loss G: {loss_G.item():.4f}')\n",
    "    torch.save(generator.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d90d0b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import ToTensor, Resize, Compose\n",
    "\n",
    "# Create a new generator\n",
    "eval_generator = UNetGenerator(3, 3).to(device)\n",
    "\n",
    "# Load the saved weights\n",
    "eval_generator.load_state_dict(torch.load('model_P2P_SSR\\generator_30.pth'))\n",
    "eval_generator.eval()  # Set the model to evaluation mode\n",
    "\n",
    "\n",
    "# Define the test dataset and the DataLoader\n",
    "test_transforms = Compose([\n",
    "    Resize((64, 64)),\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9f77a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.utils as vutils\n",
    "import os\n",
    "import numpy as np\n",
    "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
    "\n",
    "output_dir = \"generated_images(train)\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "output_dir2 = \"input_images(train)\"\n",
    "os.makedirs(output_dir2, exist_ok=True)\n",
    "\n",
    "output_dir3 = \"target_images(train)\"\n",
    "os.makedirs(output_dir3, exist_ok=True)\n",
    "\n",
    "psnrs = []\n",
    "ssims = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (image_1910, image_1970) in enumerate(train_loader):\n",
    "#         print(image_1910.shape)\n",
    "#         print(image_1970.shape)\n",
    "  \n",
    "        image_1910 = image_1910.to(device)\n",
    "        image_1970 = image_1970.to(device)\n",
    "        \n",
    "        # Generate the output image\n",
    "        generated_image = eval_generator(image_1910)\n",
    "#         print(generated_image.shape)\n",
    "        for index in range(len(image_1910)):\n",
    "            inp = image_1910[index]\n",
    "            tar = image_1970[index]\n",
    "            out = generated_image[index]\n",
    "            print(inp.shape)\n",
    "            print(tar.shape)\n",
    "            print(out.shape)\n",
    "            \n",
    "            \n",
    "            image1 = tensor_to_PIL(inp)\n",
    "            image2 = tensor_to_PIL(tar)\n",
    "            image3 = tensor_to_PIL(out)\n",
    "            \n",
    "            path1 = 'input_images(train)/patch_{num}.tif'.format(num=index)\n",
    "            path2 = 'target_images(train)/patch_{num}.tif'.format(num=index)\n",
    "            path3 = 'generated_images(train)/patch_{num}.tif'.format(num=index)\n",
    "            \n",
    "            image1.save(path1)\n",
    "            image2.save(path2)\n",
    "            image3.save(path3)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e245df84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ML] *",
   "language": "python",
   "name": "conda-env-ML-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
